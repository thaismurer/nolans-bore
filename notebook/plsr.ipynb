{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab4d0ef",
   "metadata": {},
   "source": [
    "# Partial Least Squares Regression (PLSR) for Spectral Analysis\n",
    "\n",
    "This notebook implements a Partial Least Squares Regression (PLSR) analysis for spectral data, specifically focusing on predicting Total Rare Earth Elements (TotalREE) content based on spectral measurements.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The analysis follows these main steps:\n",
    "1. Data preparation and interval-based grouping\n",
    "2. Cross-validation using group k-fold strategy\n",
    "3. PLSR model fitting and prediction\n",
    "4. Performance evaluation at sample and interval levels\n",
    "5. Variable importance analysis using VIP scores\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.x\n",
    "- Required libraries: numpy, pandas, matplotlib, scikit-learn\n",
    "- Input data: spectral measurements in CSV format with wavelength columns\n",
    "\n",
    "## How to use\n",
    "1. Ensure `hullq_interval.csv` exists with `Depth` and `interval` columns (use `add_interval_column.py` if needed).\n",
    "2. Run cells in order. Keep `RANDOM_STATE` fixed for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963d24f",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "The following cell imports all necessary Python libraries for the analysis:\n",
    "- `numpy` and `pandas` for data manipulation\n",
    "- `matplotlib` for visualization\n",
    "- `sklearn` components for model building and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c79e0",
   "metadata": {},
   "source": [
    "### Wavelength Column Detection\n",
    "Function to automatically detect wavelength columns in the dataset. It identifies columns that can be converted to floating-point numbers, assuming these represent spectral wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_wavelength_columns(df: pd.DataFrame, exclude: List[str]) -> List[str]:\n",
    "  \"\"\"Detects spectrum columns: names that can be converted to float.\n",
    "  Ex.: \"350\", \"350.5\", \"1412\". Excludes columns listed in `exclude`.\n",
    "  \"\"\"\n",
    "  wave_cols = []\n",
    "  for col in df.columns:\n",
    "    if col in exclude:\n",
    "      continue\n",
    "    try:\n",
    "      float(col)\n",
    "      wave_cols.append(col)\n",
    "    except Exception:\n",
    "      # not a numeric name -> not a wavelength\n",
    "      pass\n",
    "  if not wave_cols:\n",
    "    raise ValueError(\n",
    "      \"No spectrum columns detected. Make sure the wavelength columns \"\n",
    "      \"have numeric names (e.g., '350', '1412').\"\n",
    "    )\n",
    "  # sort by numeric value of wavelength\n",
    "  wave_cols = sorted(wave_cols, key=lambda c: float(c))\n",
    "  return wave_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45104c80",
   "metadata": {},
   "source": [
    "## Variable Importance in Projection (VIP)\n",
    "\n",
    "Implementation of VIP scores calculation for PLS regression. VIP scores indicate the importance of each predictor (wavelength) in the PLS model:\n",
    "- VIP > 1: variable is highly influential\n",
    "- VIP < 1: variable is less influential\n",
    "\n",
    "The implementation follows the standard formula used in chemometrics:\n",
    "$VIP_j = \\sqrt{\\frac{p\\sum_k(SSY_k(w_{jk}^2/\\sum_j w_{jk}^2))}{SSY_{total}}}$\n",
    "\n",
    "where:\n",
    "- $p$ is the number of predictors\n",
    "- $SSY_k$ is the sum of squares explained by component $k$\n",
    "- $w_{jk}$ is the weight of variable $j$ in component $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of VIP for PLS with one response variable (1D y).\n",
    "# Standard formula reference in chemometrics.\n",
    "\n",
    "def vip_scores(pls: PLSRegression, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Calculates VIP (Variable Importance in Projection) for 1D PLS.\n",
    "  Returns array shape (n_features,).\n",
    "  \"\"\"\n",
    "  # Adjust for 1D y case\n",
    "  if y.ndim == 2 and y.shape[1] == 1:\n",
    "    y_vec = y.ravel()\n",
    "  elif y.ndim == 1:\n",
    "    y_vec = y\n",
    "  else:\n",
    "    raise ValueError(\"VIP implemented for 1D target. Provide a single target column.\")\n",
    "\n",
    "\n",
    "  T = pls.x_scores_ # (n_samples, n_comp)\n",
    "  W = pls.x_weights_ # (n_features, n_comp)\n",
    "  Q = pls.y_loadings_ # (n_comp, 1)\n",
    "\n",
    "\n",
    "  # Sum of squares explained by component for y\n",
    "  # SSY_k = (t_k^T t_k) * (q_k^T q_k)\n",
    "  ssy = np.sum((T ** 2), axis=0) * (Q.ravel() ** 2)\n",
    "  total_ssy = np.sum(ssy)\n",
    "\n",
    "\n",
    "  # VIP_j = sqrt( p * sum_k( ssy_k * (w_jk^2 / sum_j(w_jk^2)) ) / total_ssy )\n",
    "  p = W.shape[0]\n",
    "  vip = np.zeros((p,), dtype=float)\n",
    "  for j in range(p):\n",
    "    weight = 0.0\n",
    "    for k in range(W.shape[1]):\n",
    "      wkj2 = W[j, k] ** 2\n",
    "      denom = np.sum(W[:, k] ** 2)\n",
    "      weight += ssy[k] * (wkj2 / denom)\n",
    "    vip[j] = np.sqrt((p * weight) / (total_ssy + 1e-12))\n",
    "  return vip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cf896",
   "metadata": {},
   "source": [
    "## Model Training and Cross-Validation\n",
    "\n",
    "The analysis uses group k-fold cross-validation, where groups are defined by depth intervals. This ensures that all samples from the same depth interval are kept together either in training or test set.\n",
    "\n",
    "Process:\n",
    "1. Load and preprocess the data\n",
    "2. Detect wavelength columns (predictors)\n",
    "3. Set up cross-validation folds\n",
    "4. Train PLS model on each fold\n",
    "5. Calculate performance metrics and VIP scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4377a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/processed/hullq_interval.csv'\n",
    "groupkfold_path = '../processing/folds_groupkfold.json'\n",
    "\n",
    "# ===== Data Loading and Preprocessing =====\n",
    "# Load and sort data by interval and depth\n",
    "df = pd.read_csv(data_path).sort_values([\"interval\", \"Depth\"]).reset_index(drop=True)\n",
    "df.rename(columns={'Unnamed: 0': 'Sample'}, inplace=True)\n",
    "\n",
    "# # Create 'interval' column\n",
    "# df[\"interval\"] = df['Depth'].apply(depth_to_interval_label)\n",
    "\n",
    "# df.to_csv('hullq_interval.csv', index=False)\n",
    "\n",
    "# Load pre-defined cross-validation fold indices\n",
    "with open(groupkfold_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    fold_idx = json.load(f)\n",
    "\n",
    "# ===== Feature Selection =====\n",
    "# Define non-wavelength columns to exclude\n",
    "exclude_cols = ['Sample', 'TotalREE', 'Depth', 'Mask']\n",
    "\n",
    "# Get wavelength columns (spectral features)\n",
    "wave_cols = detect_wavelength_columns(df, exclude=exclude_cols)\n",
    "target_col = [\"TotalREE\"]\n",
    "\n",
    "# ===== Data Matrix Preparation =====\n",
    "# Convert data to numpy arrays for modeling\n",
    "X = df[wave_cols].to_numpy(dtype=float)    # Spectral features\n",
    "y = df[target_col].to_numpy(dtype=float)   # Target variable\n",
    "groups = df.loc[:, \"interval\"].to_numpy()   # Group labels for CV\n",
    "depths = df[\"Depth\"].to_numpy(dtype=float)  # Depth values\n",
    "\n",
    "# ===== Model Configuration =====\n",
    "# Set number of PLS components (max 10)\n",
    "n_comp = min(10, X.shape[1])\n",
    "if n_comp < 1:\n",
    "    raise ValueError(\"Invalid n_components.\")\n",
    "\n",
    "# Initialize storage for results\n",
    "fold_rows = []       # Store metrics for each fold\n",
    "pred_rows = []       # Store predictions for each sample\n",
    "vip_accum = np.zeros(X.shape[1], dtype=float)   # Accumulate VIP scores\n",
    "coef_accum = np.zeros(X.shape[1], dtype=float)  # Accumulate coefficients\n",
    "fold_counter = 0\n",
    "\n",
    "# ===== Cross-Validation Loop =====\n",
    "for i, fdict in enumerate(fold_idx, 1):\n",
    "    print(f\"Fold: {i}\")\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_idx = np.array(fdict[\"train\"], dtype=int)\n",
    "    test_idx = np.array(fdict[\"test\"], dtype=int)\n",
    "    Xtr, Xte = X[train_idx], X[test_idx]\n",
    "    ytr, yte = y[train_idx], y[test_idx]\n",
    "    \n",
    "    print(f\"Number of samples in X_train: {len(Xtr)}\")\n",
    "    print(f\"Number of samples in X_test: {len(Xte)}\")\n",
    "\n",
    "    # Train and predict\n",
    "    pls = PLSRegression(n_components=n_comp, scale=True)\n",
    "    pls.fit(Xtr, ytr)\n",
    "    ypred = pls.predict(Xte).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(yte, ypred)\n",
    "    rmse = root_mean_squared_error(yte, ypred)\n",
    "\n",
    "    # Store test set predictions with metadata\n",
    "    test_groups_vals = groups[test_idx]\n",
    "    test_depths_vals = depths[test_idx]\n",
    "    for yt, yp, d, grp in zip(yte, ypred, test_depths_vals, test_groups_vals):\n",
    "        pred_rows.append({\n",
    "            \"fold\": i,\n",
    "            \"interval\": grp,\n",
    "            \"depth\": float(d),\n",
    "            \"y_true\": float(yt),\n",
    "            \"y_pred\": float(yp),\n",
    "        })\n",
    "\n",
    "    # Calculate and store variable importances\n",
    "    vip_fold = vip_scores(pls, Xtr, ytr)\n",
    "    coef_fold = np.abs(pls.coef_).ravel()\n",
    "    vip_accum += vip_fold\n",
    "    coef_accum += coef_fold\n",
    "    fold_counter += 1\n",
    "\n",
    "    # Store fold results\n",
    "    fold_rows.append({\n",
    "        \"fold\": i,\n",
    "        \"r2\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"train_groups\": \",\".join(sorted(set(map(str, groups[train_idx])))),\n",
    "        \"test_groups\": \",\".join(sorted(set(map(str, groups[test_idx])))),\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {i}: R2={r2:.3f}, RMSE={rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648957",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "The results are analyzed at two levels:\n",
    "\n",
    "1. **Sample Level**: Performance metrics (R² and RMSE) for individual samples across all folds\n",
    "2. **Interval Level**: Aggregated predictions for each depth interval (bag-level analysis)\n",
    "\n",
    "The visualization includes:\n",
    "- Grade profile showing real vs predicted values by depth\n",
    "- Scatter plot of real vs predicted values\n",
    "- Performance metrics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Performance Analysis =====\n",
    "\n",
    "# Calculate average metrics across folds (sample-level)\n",
    "metrics_df = pd.DataFrame(fold_rows)\n",
    "if not metrics_df.empty:\n",
    "    print(\n",
    "        f\"\\nAverage of folds (sample-level): \"\n",
    "        f\"R2={metrics_df['r2'].mean():.3f} ± {metrics_df['r2'].std():.3f} | \"\n",
    "        f\"RMSE={metrics_df['rmse'].mean():.3f} ± {metrics_df['rmse'].std():.3f}\"\n",
    "    )\n",
    "\n",
    "# ===== Interval-Level Analysis =====\n",
    "# Aggregate predictions by interval (bag-level analysis)\n",
    "preds_df = pd.DataFrame(pred_rows)\n",
    "\n",
    "# Calculate interval-level metrics\n",
    "agg = (\n",
    "    preds_df\n",
    "      .groupby(\"interval\", as_index=False)\n",
    "      .agg({\n",
    "          \"depth\": \"median\",      # Representative depth\n",
    "          \"y_true\": \"mean\",       # Average true value\n",
    "          \"y_pred\": \"mean\",       # Average prediction\n",
    "      })\n",
    "      .sort_values(\"depth\")\n",
    ")\n",
    "\n",
    "# Calculate performance metrics at interval level\n",
    "r2_bag = r2_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "rmse_bag = root_mean_squared_error(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "print(f\"OOF (bag-level): R2={r2_bag:.3f} | RMSE={rmse_bag:.3f} (interval average)\")\n",
    "\n",
    "# ===== Visualization =====\n",
    "# Plot 1: Grade Profile\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(agg[\"depth\"], agg[\"y_true\"], marker=\"o\", label=\"Real (interval)\", color='grey', linewidth=2)\n",
    "plt.plot(agg[\"depth\"], agg[\"y_pred\"], marker=\"x\", label=\"Predicted (interval)\", color='orange', linewidth=1)\n",
    "plt.xlabel(\"Depth (m)\")\n",
    "plt.ylabel(\"TotalREE\")\n",
    "plt.title(\"PLSR: real vs predict by depth (aggregated by interval)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"plsr_oof_profile.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot 2: Scatter Plot\n",
    "plt.figure()\n",
    "plt.scatter(agg[\"y_true\"], agg[\"y_pred\"], alpha=0.8, color='orange')\n",
    "# Add 1:1 line\n",
    "lims = [\n",
    "    min(agg[\"y_true\"].min(), agg[\"y_pred\"].min()),\n",
    "    max(agg[\"y_true\"].max(), agg[\"y_pred\"].max())\n",
    "]\n",
    "plt.plot(lims, lims, '--', color='gray', alpha=0.8)\n",
    "plt.xlabel(\"Real (interval)\")\n",
    "plt.ylabel(\"Predicted (interval)\")\n",
    "plt.title(\"PLSR:real vs predict (aggregated by interval)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"plsr_oof_scatter.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93b22a",
   "metadata": {},
   "source": [
    "## Variable Importance Analysis\n",
    "\n",
    "Calculate and analyze the importance of each wavelength using:\n",
    "1. VIP scores averaged across all folds\n",
    "2. Absolute coefficients from the PLS model\n",
    "\n",
    "This helps identify which wavelengths are most important for predicting TotalREE content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Variable Importance Analysis =====\n",
    "# Calculate mean importance scores across folds\n",
    "vip_mean = vip_accum / max(fold_counter, 1)\n",
    "coef_mean = coef_accum / max(fold_counter, 1)\n",
    "\n",
    "# Create DataFrame with wavelength importance metrics\n",
    "imp_df = pd.DataFrame({\n",
    "    \"wavelength\": [float(w) for w in wave_cols],\n",
    "    \"vip\": vip_mean,\n",
    "    \"coef_abs\": coef_mean\n",
    "})\n",
    "\n",
    "# Add importance rankings\n",
    "imp_df[\"rank_vip\"] = imp_df[\"vip\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "imp_df[\"rank_coef\"] = imp_df[\"coef_abs\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "\n",
    "# Sort by VIP score (most important first)\n",
    "imp_df = imp_df.sort_values(by=[\"vip\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Final Results Summary =====\n",
    "print(\"\\n==== Summary ====\")\n",
    "if not metrics_df.empty:\n",
    "    # Print detailed fold results\n",
    "    print(\"\\nCross-validation results by fold:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    # Print top wavelengths by importance\n",
    "    print(\"\\nTop 20 most important wavelengths (by VIP score):\")\n",
    "    print(imp_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a86210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
